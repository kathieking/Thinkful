{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge: The Boston marathon\n",
    "\n",
    "There is a lot of information on runners and their performance for the Boston Marathon. Pick a year (post-2012 has more info) and do some clustering.\n",
    "\n",
    "Specifically, use the tools at hand to determine which clustering solution, including number of clusters and algorithm used, is best for the marathon data. Once you have a solution you like, write a data story, including visualizations, where you teach the reader something about the Boston Marathon based on your clusters. Write up your report, including your process from start to finish, in a Jupyter notebook and submit it below.\n",
    "\n",
    "https://github.com/llimllib/bostonmarathon\n",
    "\n",
    "https://github.com/llimllib/bostomarathon/blob/master/results/2014/results.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn import metrics\n",
    "from itertools import cycle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\katec\\Thinkful\\data_collections/boston_marathon.csv', error_bad_lines=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10k</th>\n",
       "      <th>name</th>\n",
       "      <th>division</th>\n",
       "      <th>25k</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>official</th>\n",
       "      <th>bib</th>\n",
       "      <th>genderdiv</th>\n",
       "      <th>ctz</th>\n",
       "      <th>35k</th>\n",
       "      <th>overall</th>\n",
       "      <th>pace</th>\n",
       "      <th>state</th>\n",
       "      <th>30k</th>\n",
       "      <th>5k</th>\n",
       "      <th>half</th>\n",
       "      <th>20k</th>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "      <th>40k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.37</td>\n",
       "      <td>Yamamoto, Hiroyuki</td>\n",
       "      <td>8</td>\n",
       "      <td>47.67</td>\n",
       "      <td>M</td>\n",
       "      <td>47</td>\n",
       "      <td>85.25</td>\n",
       "      <td>W1</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71.4</td>\n",
       "      <td>8</td>\n",
       "      <td>3.27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59.18</td>\n",
       "      <td>8.02</td>\n",
       "      <td>39.72</td>\n",
       "      <td>37.65</td>\n",
       "      <td>JPN</td>\n",
       "      <td>Fukuoka</td>\n",
       "      <td>80.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.58</td>\n",
       "      <td>Jeptoo, Rita</td>\n",
       "      <td>1</td>\n",
       "      <td>82.43</td>\n",
       "      <td>F</td>\n",
       "      <td>33</td>\n",
       "      <td>138.95</td>\n",
       "      <td>F1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>116.37</td>\n",
       "      <td>21</td>\n",
       "      <td>5.30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.33</td>\n",
       "      <td>16.22</td>\n",
       "      <td>69.47</td>\n",
       "      <td>65.83</td>\n",
       "      <td>KEN</td>\n",
       "      <td>Eldoret</td>\n",
       "      <td>132.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.62</td>\n",
       "      <td>Van Dyk, Ernst F.</td>\n",
       "      <td>1</td>\n",
       "      <td>45.8</td>\n",
       "      <td>M</td>\n",
       "      <td>41</td>\n",
       "      <td>80.60</td>\n",
       "      <td>W2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67.42</td>\n",
       "      <td>1</td>\n",
       "      <td>3.08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.45</td>\n",
       "      <td>7.75</td>\n",
       "      <td>38.03</td>\n",
       "      <td>36.1</td>\n",
       "      <td>RSA</td>\n",
       "      <td>Paarl</td>\n",
       "      <td>76.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32.57</td>\n",
       "      <td>Dibaba, Mare</td>\n",
       "      <td>3</td>\n",
       "      <td>82.43</td>\n",
       "      <td>F</td>\n",
       "      <td>24</td>\n",
       "      <td>140.58</td>\n",
       "      <td>F2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>116.37</td>\n",
       "      <td>27</td>\n",
       "      <td>5.37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.33</td>\n",
       "      <td>16.2</td>\n",
       "      <td>69.47</td>\n",
       "      <td>65.83</td>\n",
       "      <td>ETH</td>\n",
       "      <td>Shoa</td>\n",
       "      <td>132.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.12</td>\n",
       "      <td>Hokinoue, Kota</td>\n",
       "      <td>2</td>\n",
       "      <td>46.37</td>\n",
       "      <td>M</td>\n",
       "      <td>40</td>\n",
       "      <td>81.23</td>\n",
       "      <td>W3</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67.83</td>\n",
       "      <td>2</td>\n",
       "      <td>3.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57.03</td>\n",
       "      <td>8.02</td>\n",
       "      <td>38.6</td>\n",
       "      <td>36.58</td>\n",
       "      <td>JPN</td>\n",
       "      <td>Nogata Fukuoka</td>\n",
       "      <td>76.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     10k                name  division    25k gender  age  official bib  \\\n",
       "0  17.37  Yamamoto, Hiroyuki         8  47.67      M   47     85.25  W1   \n",
       "1  32.58        Jeptoo, Rita         1  82.43      F   33    138.95  F1   \n",
       "2  16.62   Van Dyk, Ernst F.         1   45.8      M   41     80.60  W2   \n",
       "3  32.57        Dibaba, Mare         3  82.43      F   24    140.58  F2   \n",
       "4  17.12      Hokinoue, Kota         2  46.37      M   40     81.23  W3   \n",
       "\n",
       "   genderdiv  ctz     35k  overall  pace state    30k     5k   half    20k  \\\n",
       "0          8  NaN    71.4        8  3.27   NaN  59.18   8.02  39.72  37.65   \n",
       "1          1  NaN  116.37       21  5.30   NaN  99.33  16.22  69.47  65.83   \n",
       "2          1  NaN   67.42        1  3.08   NaN  56.45   7.75  38.03   36.1   \n",
       "3          3  NaN  116.37       27  5.37   NaN  99.33   16.2  69.47  65.83   \n",
       "4          2  NaN   67.83        2  3.10   NaN  57.03   8.02   38.6  36.58   \n",
       "\n",
       "  country            city     40k  \n",
       "0     JPN         Fukuoka   80.43  \n",
       "1     KEN         Eldoret   132.1  \n",
       "2     RSA           Paarl    76.1  \n",
       "3     ETH            Shoa  132.95  \n",
       "4     JPN  Nogata Fukuoka   76.72  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31984 entries, 0 to 31983\n",
      "Data columns (total 21 columns):\n",
      "10k          31984 non-null object\n",
      "name         31984 non-null object\n",
      "division     31984 non-null int64\n",
      "25k          31984 non-null object\n",
      "gender       31984 non-null object\n",
      "age          31984 non-null int64\n",
      "official     31984 non-null float64\n",
      "bib          31984 non-null object\n",
      "genderdiv    31984 non-null int64\n",
      "ctz          1244 non-null object\n",
      "35k          31984 non-null object\n",
      "overall      31984 non-null int64\n",
      "pace         31984 non-null float64\n",
      "state        29408 non-null object\n",
      "30k          31984 non-null object\n",
      "5k           31984 non-null object\n",
      "half         31984 non-null object\n",
      "20k          31984 non-null object\n",
      "country      31984 non-null object\n",
      "city         31983 non-null object\n",
      "40k          31984 non-null object\n",
      "dtypes: float64(2), int64(4), object(15)\n",
      "memory usage: 5.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to numeric\n",
    "cols = ['10k', '25k', '35k', '30k', '5k', 'half', '20k', '40k']\n",
    "df[cols] = df[cols].apply(pd.to_numeric, errors = 'coerce', downcast = 'float')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10k             50\n",
       "name             0\n",
       "division         0\n",
       "25k            216\n",
       "gender           0\n",
       "age              0\n",
       "official         0\n",
       "bib              0\n",
       "genderdiv        0\n",
       "ctz          30740\n",
       "35k             34\n",
       "overall          0\n",
       "pace             0\n",
       "state         2576\n",
       "30k             40\n",
       "5k              52\n",
       "half            72\n",
       "20k             51\n",
       "country          0\n",
       "city             1\n",
       "40k             39\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop 'ctz' column, empty\n",
    "df = df.drop(['ctz'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill missimg 'state', 'city' with 'Not available'\n",
    "fill_list = ['state', 'city']\n",
    "for column in fill_list:\n",
    "        df.loc[:, column] = df.loc[:, column].fillna(value= 'Not available') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion factor\n",
    "26 miles 385 yards = 42.195 km;  \n",
    "1 = 1.60934"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10k</th>\n",
       "      <th>name</th>\n",
       "      <th>division</th>\n",
       "      <th>25k</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>official</th>\n",
       "      <th>bib</th>\n",
       "      <th>genderdiv</th>\n",
       "      <th>35k</th>\n",
       "      <th>overall</th>\n",
       "      <th>pace</th>\n",
       "      <th>state</th>\n",
       "      <th>30k</th>\n",
       "      <th>5k</th>\n",
       "      <th>half</th>\n",
       "      <th>20k</th>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "      <th>40k</th>\n",
       "      <th>pace_km</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.370001</td>\n",
       "      <td>Yamamoto, Hiroyuki</td>\n",
       "      <td>8</td>\n",
       "      <td>47.669998</td>\n",
       "      <td>M</td>\n",
       "      <td>47</td>\n",
       "      <td>85.25</td>\n",
       "      <td>W1</td>\n",
       "      <td>8</td>\n",
       "      <td>71.400002</td>\n",
       "      <td>8</td>\n",
       "      <td>3.27</td>\n",
       "      <td>Not available</td>\n",
       "      <td>59.180000</td>\n",
       "      <td>8.020000</td>\n",
       "      <td>39.720001</td>\n",
       "      <td>37.650002</td>\n",
       "      <td>JPN</td>\n",
       "      <td>Fukuoka</td>\n",
       "      <td>80.430000</td>\n",
       "      <td>5.262542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.580002</td>\n",
       "      <td>Jeptoo, Rita</td>\n",
       "      <td>1</td>\n",
       "      <td>82.430000</td>\n",
       "      <td>F</td>\n",
       "      <td>33</td>\n",
       "      <td>138.95</td>\n",
       "      <td>F1</td>\n",
       "      <td>1</td>\n",
       "      <td>116.370003</td>\n",
       "      <td>21</td>\n",
       "      <td>5.30</td>\n",
       "      <td>Not available</td>\n",
       "      <td>99.330002</td>\n",
       "      <td>16.219999</td>\n",
       "      <td>69.470001</td>\n",
       "      <td>65.830002</td>\n",
       "      <td>KEN</td>\n",
       "      <td>Eldoret</td>\n",
       "      <td>132.100006</td>\n",
       "      <td>8.529502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.620001</td>\n",
       "      <td>Van Dyk, Ernst F.</td>\n",
       "      <td>1</td>\n",
       "      <td>45.799999</td>\n",
       "      <td>M</td>\n",
       "      <td>41</td>\n",
       "      <td>80.60</td>\n",
       "      <td>W2</td>\n",
       "      <td>1</td>\n",
       "      <td>67.419998</td>\n",
       "      <td>1</td>\n",
       "      <td>3.08</td>\n",
       "      <td>Not available</td>\n",
       "      <td>56.450001</td>\n",
       "      <td>7.750000</td>\n",
       "      <td>38.029999</td>\n",
       "      <td>36.099998</td>\n",
       "      <td>RSA</td>\n",
       "      <td>Paarl</td>\n",
       "      <td>76.099998</td>\n",
       "      <td>4.956767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32.570000</td>\n",
       "      <td>Dibaba, Mare</td>\n",
       "      <td>3</td>\n",
       "      <td>82.430000</td>\n",
       "      <td>F</td>\n",
       "      <td>24</td>\n",
       "      <td>140.58</td>\n",
       "      <td>F2</td>\n",
       "      <td>3</td>\n",
       "      <td>116.370003</td>\n",
       "      <td>27</td>\n",
       "      <td>5.37</td>\n",
       "      <td>Not available</td>\n",
       "      <td>99.330002</td>\n",
       "      <td>16.200001</td>\n",
       "      <td>69.470001</td>\n",
       "      <td>65.830002</td>\n",
       "      <td>ETH</td>\n",
       "      <td>Shoa</td>\n",
       "      <td>132.949997</td>\n",
       "      <td>8.642156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.120001</td>\n",
       "      <td>Hokinoue, Kota</td>\n",
       "      <td>2</td>\n",
       "      <td>46.369999</td>\n",
       "      <td>M</td>\n",
       "      <td>40</td>\n",
       "      <td>81.23</td>\n",
       "      <td>W3</td>\n",
       "      <td>2</td>\n",
       "      <td>67.830002</td>\n",
       "      <td>2</td>\n",
       "      <td>3.10</td>\n",
       "      <td>Not available</td>\n",
       "      <td>57.029999</td>\n",
       "      <td>8.020000</td>\n",
       "      <td>38.599998</td>\n",
       "      <td>36.580002</td>\n",
       "      <td>JPN</td>\n",
       "      <td>Nogata Fukuoka</td>\n",
       "      <td>76.720001</td>\n",
       "      <td>4.988954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         10k                name  division        25k gender  age  official  \\\n",
       "0  17.370001  Yamamoto, Hiroyuki         8  47.669998      M   47     85.25   \n",
       "1  32.580002        Jeptoo, Rita         1  82.430000      F   33    138.95   \n",
       "2  16.620001   Van Dyk, Ernst F.         1  45.799999      M   41     80.60   \n",
       "3  32.570000        Dibaba, Mare         3  82.430000      F   24    140.58   \n",
       "4  17.120001      Hokinoue, Kota         2  46.369999      M   40     81.23   \n",
       "\n",
       "  bib  genderdiv         35k  overall  pace          state        30k  \\\n",
       "0  W1          8   71.400002        8  3.27  Not available  59.180000   \n",
       "1  F1          1  116.370003       21  5.30  Not available  99.330002   \n",
       "2  W2          1   67.419998        1  3.08  Not available  56.450001   \n",
       "3  F2          3  116.370003       27  5.37  Not available  99.330002   \n",
       "4  W3          2   67.830002        2  3.10  Not available  57.029999   \n",
       "\n",
       "          5k       half        20k country            city         40k  \\\n",
       "0   8.020000  39.720001  37.650002     JPN         Fukuoka   80.430000   \n",
       "1  16.219999  69.470001  65.830002     KEN         Eldoret  132.100006   \n",
       "2   7.750000  38.029999  36.099998     RSA           Paarl   76.099998   \n",
       "3  16.200001  69.470001  65.830002     ETH            Shoa  132.949997   \n",
       "4   8.020000  38.599998  36.580002     JPN  Nogata Fukuoka   76.720001   \n",
       "\n",
       "    pace_km  \n",
       "0  5.262542  \n",
       "1  8.529502  \n",
       "2  4.956767  \n",
       "3  8.642156  \n",
       "4  4.988954  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert pace from miles to km (factor = 1.60934)\n",
    "df['pace_km'] = df['pace']* 1.60934\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    31768.000000\n",
       "mean       133.611832\n",
       "std         25.596964\n",
       "min         45.799999\n",
       "25%        115.470001\n",
       "50%        128.875000\n",
       "75%        147.735001\n",
       "max        289.019989\n",
       "Name: 25k, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['25k'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fill for single: \n",
    "df['5k'] = df['5k'].fillna(5 * df['pace_km'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill missing marker time values by \n",
    "#multiply factor_list by 'pace_km'\n",
    "fill2_list = ['5k', '10k', '20k', '25k', '30k', '35k', '40k']\n",
    "\n",
    "for c in fill2_list:\n",
    "        df[c].fillna(df['pace_km'] * int(c.replace('k', '')), inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10k           0\n",
       "name          0\n",
       "division      0\n",
       "25k           0\n",
       "gender        0\n",
       "age           0\n",
       "official      0\n",
       "bib           0\n",
       "genderdiv     0\n",
       "35k           0\n",
       "overall       0\n",
       "pace          0\n",
       "state         0\n",
       "30k           0\n",
       "5k            0\n",
       "half         72\n",
       "20k           0\n",
       "country       0\n",
       "city          0\n",
       "40k           0\n",
       "pace_km       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill missing marker time values by \n",
    "#dividing 'official' in half\n",
    "#fill3_list = ['half']\n",
    "\n",
    "for column in fill_list:\n",
    "        df['half'] = df['half'].fillna(df['official']/2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10k          0\n",
       "name         0\n",
       "division     0\n",
       "25k          0\n",
       "gender       0\n",
       "age          0\n",
       "official     0\n",
       "bib          0\n",
       "genderdiv    0\n",
       "35k          0\n",
       "overall      0\n",
       "pace         0\n",
       "state        0\n",
       "30k          0\n",
       "5k           0\n",
       "half         0\n",
       "20k          0\n",
       "country      0\n",
       "city         0\n",
       "40k          0\n",
       "pace_km      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31984 entries, 0 to 31983\n",
      "Data columns (total 21 columns):\n",
      "10k          31984 non-null float32\n",
      "name         31984 non-null object\n",
      "division     31984 non-null int64\n",
      "25k          31984 non-null float32\n",
      "gender       31984 non-null object\n",
      "age          31984 non-null int64\n",
      "official     31984 non-null float64\n",
      "bib          31984 non-null object\n",
      "genderdiv    31984 non-null int64\n",
      "35k          31984 non-null float32\n",
      "overall      31984 non-null int64\n",
      "pace         31984 non-null float64\n",
      "state        31984 non-null object\n",
      "30k          31984 non-null float32\n",
      "5k           31984 non-null float32\n",
      "half         31984 non-null float32\n",
      "20k          31984 non-null float32\n",
      "country      31984 non-null object\n",
      "city         31984 non-null object\n",
      "40k          31984 non-null float32\n",
      "pace_km      31984 non-null float64\n",
      "dtypes: float32(8), float64(3), int64(4), object(6)\n",
      "memory usage: 4.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features = ['10k', 'division', '25k', 'age', 'official', \n",
    "                       'genderdiv', '35k', 'overall', 'pace', '30k', \n",
    "                       '5k', 'half', '20k', '40k', 'pace_km']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10k</th>\n",
       "      <th>division</th>\n",
       "      <th>25k</th>\n",
       "      <th>age</th>\n",
       "      <th>official</th>\n",
       "      <th>genderdiv</th>\n",
       "      <th>35k</th>\n",
       "      <th>overall</th>\n",
       "      <th>pace</th>\n",
       "      <th>30k</th>\n",
       "      <th>5k</th>\n",
       "      <th>half</th>\n",
       "      <th>20k</th>\n",
       "      <th>40k</th>\n",
       "      <th>pace_km</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>31984.000000</td>\n",
       "      <td>31984.000000</td>\n",
       "      <td>31984.000000</td>\n",
       "      <td>31984.000000</td>\n",
       "      <td>31984.000000</td>\n",
       "      <td>31984.000000</td>\n",
       "      <td>31984.000000</td>\n",
       "      <td>31984.000000</td>\n",
       "      <td>31984.000000</td>\n",
       "      <td>31984.000000</td>\n",
       "      <td>31984.000000</td>\n",
       "      <td>31984.000000</td>\n",
       "      <td>31984.000000</td>\n",
       "      <td>31984.000000</td>\n",
       "      <td>31984.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>51.980061</td>\n",
       "      <td>1932.563032</td>\n",
       "      <td>136.680176</td>\n",
       "      <td>42.407079</td>\n",
       "      <td>242.997314</td>\n",
       "      <td>8051.044741</td>\n",
       "      <td>197.289169</td>\n",
       "      <td>15939.587825</td>\n",
       "      <td>9.275658</td>\n",
       "      <td>165.036575</td>\n",
       "      <td>25.843662</td>\n",
       "      <td>111.700729</td>\n",
       "      <td>106.134109</td>\n",
       "      <td>229.882507</td>\n",
       "      <td>14.927688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.160045</td>\n",
       "      <td>1715.228694</td>\n",
       "      <td>45.737686</td>\n",
       "      <td>11.316496</td>\n",
       "      <td>52.300431</td>\n",
       "      <td>4754.005626</td>\n",
       "      <td>44.339222</td>\n",
       "      <td>9232.978224</td>\n",
       "      <td>1.992486</td>\n",
       "      <td>36.733776</td>\n",
       "      <td>4.883998</td>\n",
       "      <td>21.084845</td>\n",
       "      <td>22.788675</td>\n",
       "      <td>55.603344</td>\n",
       "      <td>3.206588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>16.620001</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>45.799999</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>80.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>67.419998</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.080000</td>\n",
       "      <td>56.450001</td>\n",
       "      <td>7.750000</td>\n",
       "      <td>38.029999</td>\n",
       "      <td>36.099998</td>\n",
       "      <td>76.099998</td>\n",
       "      <td>4.956767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>45.419998</td>\n",
       "      <td>610.000000</td>\n",
       "      <td>115.570000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>205.527500</td>\n",
       "      <td>3972.000000</td>\n",
       "      <td>167.149994</td>\n",
       "      <td>7943.750000</td>\n",
       "      <td>7.850000</td>\n",
       "      <td>140.694996</td>\n",
       "      <td>22.650000</td>\n",
       "      <td>96.870003</td>\n",
       "      <td>91.769997</td>\n",
       "      <td>193.830002</td>\n",
       "      <td>12.633319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>50.349998</td>\n",
       "      <td>1425.000000</td>\n",
       "      <td>129.070007</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>232.370000</td>\n",
       "      <td>7970.000000</td>\n",
       "      <td>188.220001</td>\n",
       "      <td>15939.500000</td>\n",
       "      <td>8.870000</td>\n",
       "      <td>157.779999</td>\n",
       "      <td>25.100000</td>\n",
       "      <td>107.870003</td>\n",
       "      <td>102.180000</td>\n",
       "      <td>218.899994</td>\n",
       "      <td>14.274846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>57.200001</td>\n",
       "      <td>2611.000000</td>\n",
       "      <td>148.380005</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>273.235000</td>\n",
       "      <td>11968.000000</td>\n",
       "      <td>220.279999</td>\n",
       "      <td>23935.250000</td>\n",
       "      <td>10.430000</td>\n",
       "      <td>183.080002</td>\n",
       "      <td>28.500000</td>\n",
       "      <td>123.120003</td>\n",
       "      <td>116.599998</td>\n",
       "      <td>257.649994</td>\n",
       "      <td>16.785416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>261.195892</td>\n",
       "      <td>6979.000000</td>\n",
       "      <td>710.925964</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>538.880000</td>\n",
       "      <td>17575.000000</td>\n",
       "      <td>974.455383</td>\n",
       "      <td>31931.000000</td>\n",
       "      <td>20.570000</td>\n",
       "      <td>853.111145</td>\n",
       "      <td>130.597946</td>\n",
       "      <td>236.669998</td>\n",
       "      <td>568.740784</td>\n",
       "      <td>1137.481567</td>\n",
       "      <td>33.104124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                10k      division           25k           age      official  \\\n",
       "count  31984.000000  31984.000000  31984.000000  31984.000000  31984.000000   \n",
       "mean      51.980061   1932.563032    136.680176     42.407079    242.997314   \n",
       "std       10.160045   1715.228694     45.737686     11.316496     52.300431   \n",
       "min       16.620001      1.000000     45.799999     18.000000     80.600000   \n",
       "25%       45.419998    610.000000    115.570000     33.000000    205.527500   \n",
       "50%       50.349998   1425.000000    129.070007     42.000000    232.370000   \n",
       "75%       57.200001   2611.000000    148.380005     50.000000    273.235000   \n",
       "max      261.195892   6979.000000    710.925964     81.000000    538.880000   \n",
       "\n",
       "          genderdiv           35k       overall          pace           30k  \\\n",
       "count  31984.000000  31984.000000  31984.000000  31984.000000  31984.000000   \n",
       "mean    8051.044741    197.289169  15939.587825      9.275658    165.036575   \n",
       "std     4754.005626     44.339222   9232.978224      1.992486     36.733776   \n",
       "min        1.000000     67.419998      1.000000      3.080000     56.450001   \n",
       "25%     3972.000000    167.149994   7943.750000      7.850000    140.694996   \n",
       "50%     7970.000000    188.220001  15939.500000      8.870000    157.779999   \n",
       "75%    11968.000000    220.279999  23935.250000     10.430000    183.080002   \n",
       "max    17575.000000    974.455383  31931.000000     20.570000    853.111145   \n",
       "\n",
       "                 5k          half           20k           40k       pace_km  \n",
       "count  31984.000000  31984.000000  31984.000000  31984.000000  31984.000000  \n",
       "mean      25.843662    111.700729    106.134109    229.882507     14.927688  \n",
       "std        4.883998     21.084845     22.788675     55.603344      3.206588  \n",
       "min        7.750000     38.029999     36.099998     76.099998      4.956767  \n",
       "25%       22.650000     96.870003     91.769997    193.830002     12.633319  \n",
       "50%       25.100000    107.870003    102.180000    218.899994     14.274846  \n",
       "75%       28.500000    123.120003    116.599998    257.649994     16.785416  \n",
       "max      130.597946    236.669998    568.740784   1137.481567     33.104124  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[continuous_features].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_features = ['gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10k</th>\n",
       "      <th>name</th>\n",
       "      <th>division</th>\n",
       "      <th>25k</th>\n",
       "      <th>age</th>\n",
       "      <th>official</th>\n",
       "      <th>bib</th>\n",
       "      <th>genderdiv</th>\n",
       "      <th>35k</th>\n",
       "      <th>overall</th>\n",
       "      <th>pace</th>\n",
       "      <th>state</th>\n",
       "      <th>30k</th>\n",
       "      <th>5k</th>\n",
       "      <th>half</th>\n",
       "      <th>20k</th>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "      <th>40k</th>\n",
       "      <th>pace_km</th>\n",
       "      <th>gender_M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.370001</td>\n",
       "      <td>Yamamoto, Hiroyuki</td>\n",
       "      <td>8</td>\n",
       "      <td>47.669998</td>\n",
       "      <td>47</td>\n",
       "      <td>85.25</td>\n",
       "      <td>W1</td>\n",
       "      <td>8</td>\n",
       "      <td>71.400002</td>\n",
       "      <td>8</td>\n",
       "      <td>3.27</td>\n",
       "      <td>Not available</td>\n",
       "      <td>59.180000</td>\n",
       "      <td>8.020000</td>\n",
       "      <td>39.720001</td>\n",
       "      <td>37.650002</td>\n",
       "      <td>JPN</td>\n",
       "      <td>Fukuoka</td>\n",
       "      <td>80.430000</td>\n",
       "      <td>5.262542</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.580002</td>\n",
       "      <td>Jeptoo, Rita</td>\n",
       "      <td>1</td>\n",
       "      <td>82.430000</td>\n",
       "      <td>33</td>\n",
       "      <td>138.95</td>\n",
       "      <td>F1</td>\n",
       "      <td>1</td>\n",
       "      <td>116.370003</td>\n",
       "      <td>21</td>\n",
       "      <td>5.30</td>\n",
       "      <td>Not available</td>\n",
       "      <td>99.330002</td>\n",
       "      <td>16.219999</td>\n",
       "      <td>69.470001</td>\n",
       "      <td>65.830002</td>\n",
       "      <td>KEN</td>\n",
       "      <td>Eldoret</td>\n",
       "      <td>132.100006</td>\n",
       "      <td>8.529502</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.620001</td>\n",
       "      <td>Van Dyk, Ernst F.</td>\n",
       "      <td>1</td>\n",
       "      <td>45.799999</td>\n",
       "      <td>41</td>\n",
       "      <td>80.60</td>\n",
       "      <td>W2</td>\n",
       "      <td>1</td>\n",
       "      <td>67.419998</td>\n",
       "      <td>1</td>\n",
       "      <td>3.08</td>\n",
       "      <td>Not available</td>\n",
       "      <td>56.450001</td>\n",
       "      <td>7.750000</td>\n",
       "      <td>38.029999</td>\n",
       "      <td>36.099998</td>\n",
       "      <td>RSA</td>\n",
       "      <td>Paarl</td>\n",
       "      <td>76.099998</td>\n",
       "      <td>4.956767</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32.570000</td>\n",
       "      <td>Dibaba, Mare</td>\n",
       "      <td>3</td>\n",
       "      <td>82.430000</td>\n",
       "      <td>24</td>\n",
       "      <td>140.58</td>\n",
       "      <td>F2</td>\n",
       "      <td>3</td>\n",
       "      <td>116.370003</td>\n",
       "      <td>27</td>\n",
       "      <td>5.37</td>\n",
       "      <td>Not available</td>\n",
       "      <td>99.330002</td>\n",
       "      <td>16.200001</td>\n",
       "      <td>69.470001</td>\n",
       "      <td>65.830002</td>\n",
       "      <td>ETH</td>\n",
       "      <td>Shoa</td>\n",
       "      <td>132.949997</td>\n",
       "      <td>8.642156</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.120001</td>\n",
       "      <td>Hokinoue, Kota</td>\n",
       "      <td>2</td>\n",
       "      <td>46.369999</td>\n",
       "      <td>40</td>\n",
       "      <td>81.23</td>\n",
       "      <td>W3</td>\n",
       "      <td>2</td>\n",
       "      <td>67.830002</td>\n",
       "      <td>2</td>\n",
       "      <td>3.10</td>\n",
       "      <td>Not available</td>\n",
       "      <td>57.029999</td>\n",
       "      <td>8.020000</td>\n",
       "      <td>38.599998</td>\n",
       "      <td>36.580002</td>\n",
       "      <td>JPN</td>\n",
       "      <td>Nogata Fukuoka</td>\n",
       "      <td>76.720001</td>\n",
       "      <td>4.988954</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         10k                name  division        25k  age  official bib  \\\n",
       "0  17.370001  Yamamoto, Hiroyuki         8  47.669998   47     85.25  W1   \n",
       "1  32.580002        Jeptoo, Rita         1  82.430000   33    138.95  F1   \n",
       "2  16.620001   Van Dyk, Ernst F.         1  45.799999   41     80.60  W2   \n",
       "3  32.570000        Dibaba, Mare         3  82.430000   24    140.58  F2   \n",
       "4  17.120001      Hokinoue, Kota         2  46.369999   40     81.23  W3   \n",
       "\n",
       "   genderdiv         35k  overall  pace          state        30k         5k  \\\n",
       "0          8   71.400002        8  3.27  Not available  59.180000   8.020000   \n",
       "1          1  116.370003       21  5.30  Not available  99.330002  16.219999   \n",
       "2          1   67.419998        1  3.08  Not available  56.450001   7.750000   \n",
       "3          3  116.370003       27  5.37  Not available  99.330002  16.200001   \n",
       "4          2   67.830002        2  3.10  Not available  57.029999   8.020000   \n",
       "\n",
       "        half        20k country            city         40k   pace_km  \\\n",
       "0  39.720001  37.650002     JPN         Fukuoka   80.430000  5.262542   \n",
       "1  69.470001  65.830002     KEN         Eldoret  132.100006  8.529502   \n",
       "2  38.029999  36.099998     RSA           Paarl   76.099998  4.956767   \n",
       "3  69.470001  65.830002     ETH            Shoa  132.949997  8.642156   \n",
       "4  38.599998  36.580002     JPN  Nogata Fukuoka   76.720001  4.988954   \n",
       "\n",
       "   gender_M  \n",
       "0         1  \n",
       "1         0  \n",
       "2         1  \n",
       "3         0  \n",
       "4         1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for column in binary_features:\n",
    "   \n",
    "for column in binary_features:\n",
    "    df = pd.concat([df,pd.get_dummies(df[column], prefix=column, drop_first=True)], axis=1)\n",
    "    df = df.drop(['gender'], axis=1)\n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['name', 'bib', 'state', 'country',\n",
    "                       'city',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>bib</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>31984</td>\n",
       "      <td>31984</td>\n",
       "      <td>31984</td>\n",
       "      <td>31984</td>\n",
       "      <td>31984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>31915</td>\n",
       "      <td>31984</td>\n",
       "      <td>69</td>\n",
       "      <td>78</td>\n",
       "      <td>5935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Mitchell, Tom</td>\n",
       "      <td>9946</td>\n",
       "      <td>MA</td>\n",
       "      <td>USA</td>\n",
       "      <td>Boston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7587</td>\n",
       "      <td>27233</td>\n",
       "      <td>1034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 name    bib  state country    city\n",
       "count           31984  31984  31984   31984   31984\n",
       "unique          31915  31984     69      78    5935\n",
       "top     Mitchell, Tom   9946     MA     USA  Boston\n",
       "freq                2      1   7587   27233    1034"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[categorical_features].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    31984.000000\n",
       "mean         0.550807\n",
       "std          0.497420\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          1.000000\n",
       "75%          1.000000\n",
       "max          1.000000\n",
       "Name: gender_M, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['gender_M'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Yamamoto, Hiroyuki'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-77d1aa449b83>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdf_transfmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 334\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    360\u001b[0m         X = check_array(X, copy=self.copy, warn_on_dtype=True,\n\u001b[0;32m    361\u001b[0m                         \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m                         force_all_finite=\"allow-nan\")\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[0mdata_min\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnanmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    525\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 527\u001b[1;33m                 \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    528\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \"\"\"\n\u001b[1;32m--> 538\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Yamamoto, Hiroyuki'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mms = MinMaxScaler()\n",
    "mms.fit(df)\n",
    "df_transfmd = mms.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Yamamoto, Hiroyuki'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-db330cee6643>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_transfmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m         X = check_array(X, copy=self.copy, dtype=FLOAT_DTYPES,\n\u001b[1;32m--> 396\u001b[1;33m                         force_all_finite=\"allow-nan\")\n\u001b[0m\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    525\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 527\u001b[1;33m                 \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    528\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \"\"\"\n\u001b[1;32m--> 538\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Yamamoto, Hiroyuki'"
     ]
    }
   ],
   "source": [
    "df_transfmd = mms.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Yamamoto, Hiroyuki'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-8a7b35844d3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdf_transfmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 645\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    667\u001b[0m         X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,\n\u001b[0;32m    668\u001b[0m                         \u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m                         force_all_finite='allow-nan')\n\u001b[0m\u001b[0;32m    670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m         \u001b[1;31m# Even in the case of `with_mean=False`, we update the mean anyway\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    525\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 527\u001b[1;33m                 \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    528\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \"\"\"\n\u001b[1;32m--> 538\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Yamamoto, Hiroyuki'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sts = StandardScaler()\n",
    "sts.fit(df)\n",
    "df_transfmd = sts.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sklearn.cluster.KMeans(n_clusters=8, \n",
    "                             init=’k-means++’, \n",
    "                             n_init=10, \n",
    "                             max_iter=300, \n",
    "                             tol=0.0001, \n",
    "                             precompute_distances=’auto’,\n",
    "                             verbose=0, \n",
    "                             random_state=None, \n",
    "                             copy_x=True, \n",
    "                             n_jobs=None, \n",
    "                             algorithm=’auto’)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features and the outcome.\n",
    "\n",
    "X = df.iloc[:, :13]\n",
    "y = df.iloc[:, 13]\n",
    "\n",
    "# Replace missing values (marked by ?) with a 0.\n",
    "X = X.replace(to_replace='?', value=0)\n",
    "\n",
    "# Binarize y so that 1 means heart disease diagnosis and 0 means no diagnosis.\n",
    "y = np.where(y > 0, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data.\n",
    "X_norm = normalize(X)\n",
    "\n",
    "# DO NOT DO THIS UNTIL READY TO GRAPH Reduce it to two components.\n",
    "X_pca = PCA(2).fit_transform(X_norm)\n",
    "\n",
    "# Calculate predicted values.\n",
    "y_pred = KMeans(n_clusters=2, random_state=42).fit_predict(X_pca)\n",
    "\n",
    "# Plot the solution.\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_pred)\n",
    "plt.show()\n",
    "\n",
    "# Check the solution against the data.\n",
    "print('Comparing k-means clusters against the data:')\n",
    "print(pd.crosstab(y_pred, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each batch will be made up of 200 data points.\n",
    "minibatchkmeans = MiniBatchKMeans(\n",
    "    init='random',\n",
    "    n_clusters=2,\n",
    "    batch_size=200)\n",
    "minibatchkmeans.fit(X_pca)\n",
    "\n",
    "# Predict new MiniBatch cluster memberships.\n",
    "predict_mini = minibatchkmeans.predict(X_pca)\n",
    "\n",
    "# Check the MiniBatch model against our earlier one.\n",
    "print('Comparing k-means and mini batch k-means solutions:')\n",
    "print(pd.crosstab(predict_mini, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Each batch will be made up of 200 data points.\n",
    "minibatchkmeans = MiniBatchKMeans(\n",
    "    init='random',\n",
    "    n_clusters=2,\n",
    "    batch_size=200)\n",
    "minibatchkmeans.fit(X_pca)\n",
    "\n",
    "# Predict new MiniBatch cluster memberships.\n",
    "predict_mini = minibatchkmeans.predict(X_pca)\n",
    "\n",
    "# Check the MiniBatch model against our earlier one.\n",
    "print('Comparing k-means and mini batch k-means solutions:')\n",
    "print(pd.crosstab(predict_mini, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= -2 * np.random.rand(100,2)\n",
    "X1 = 1 + 2 * np.random.rand(50,2)\n",
    "X[50:100, :] = X1\n",
    "plt.scatter(X[ : , 0], X[ :, 1], s = 50, c = ‘b’)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "Kmean = KMeans(n_clusters=2)\n",
    "Kmean.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the centroid\n",
    "Kmean.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the cluster centroids (using green and red color)\n",
    "plt.scatter(X[ : , 0], X[ : , 1], s =50, c=’b’)\n",
    "plt.scatter(-0.94665068, -0.97138368, s=200, c=’g’, marker=’s’)\n",
    "plt.scatter(2.01559419, 2.02597093, s=200, c=’r’, marker=’s’)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(\n",
    "    n_clusters=3, init='random',\n",
    "    n_init=10, max_iter=300, \n",
    "    tol=1e-04, random_state=0\n",
    ")\n",
    "y_km = km.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the 3 clusters\n",
    "plt.scatter(\n",
    "    X[y_km == 0, 0], X[y_km == 0, 1],\n",
    "    s=50, c='lightgreen',\n",
    "    marker='s', edgecolor='black',\n",
    "    label='cluster 1'\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    X[y_km == 1, 0], X[y_km == 1, 1],\n",
    "    s=50, c='orange',\n",
    "    marker='o', edgecolor='black',\n",
    "    label='cluster 2'\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    X[y_km == 2, 0], X[y_km == 2, 1],\n",
    "    s=50, c='lightblue',\n",
    "    marker='v', edgecolor='black',\n",
    "    label='cluster 3'\n",
    ")\n",
    "\n",
    "# plot the centroids\n",
    "plt.scatter(\n",
    "    km.cluster_centers_[:, 0], km.cluster_centers_[:, 1],\n",
    "    s=250, marker='*',\n",
    "    c='red', edgecolor='black',\n",
    "    label='centroids'\n",
    ")\n",
    "plt.legend(scatterpoints=1)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elbow method\n",
    "# calculate distortion for a range of number of cluster\n",
    "distortions = []\n",
    "for i in range(1, 11):\n",
    "    km = KMeans(\n",
    "        n_clusters=i, init='random',\n",
    "        n_init=10, max_iter=300,\n",
    "        tol=1e-04, random_state=0\n",
    "    )\n",
    "    km.fit(X)\n",
    "    distortions.append(km.inertia_)\n",
    "\n",
    "# plot\n",
    "plt.plot(range(1, 11), distortions, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Distortion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "class Kmeans:\n",
    "    '''Implementing Kmeans algorithm.'''\n",
    "\n",
    "    def __init__(self, n_clusters, max_iter=100, random_state=123):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def initializ_centroids(self, X):\n",
    "        np.random.RandomState(self.random_state)\n",
    "        random_idx = np.random.permutation(X.shape[0])\n",
    "        centroids = X[random_idx[:self.n_clusters]]\n",
    "        return centroids\n",
    "\n",
    "    def compute_centroids(self, X, labels):\n",
    "        centroids = np.zeros((self.n_clusters, X.shape[1]))\n",
    "        for k in range(self.n_clusters):\n",
    "            centroids[k, :] = np.mean(X[labels == k, :], axis=0)\n",
    "        return centroids\n",
    "\n",
    "    def compute_distance(self, X, centroids):\n",
    "        distance = np.zeros((X.shape[0], self.n_clusters))\n",
    "        for k in range(self.n_clusters):\n",
    "            row_norm = norm(X - centroids[k, :], axis=1)\n",
    "            distance[:, k] = np.square(row_norm)\n",
    "        return distance\n",
    "\n",
    "    def find_closest_cluster(self, distance):\n",
    "        return np.argmin(distance, axis=1)\n",
    "\n",
    "    def compute_sse(self, X, labels, centroids):\n",
    "        distance = np.zeros(X.shape[0])\n",
    "        for k in range(self.n_clusters):\n",
    "            distance[labels == k] = norm(X[labels == k] - centroids[k], axis=1)\n",
    "        return np.sum(np.square(distance))\n",
    "    \n",
    "    def fit(self, X):\n",
    "        self.centroids = self.initializ_centroids(X)\n",
    "        for i in range(self.max_iter):\n",
    "            old_centroids = self.centroids\n",
    "            distance = self.compute_distance(X, old_centroids)\n",
    "            self.labels = self.find_closest_cluster(distance)\n",
    "            self.centroids = self.compute_centroids(X, self.labels)\n",
    "            if np.all(old_centroids == self.centroids):\n",
    "                break\n",
    "        self.error = self.compute_sse(X, self.labels, self.centroids)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        distance = self.compute_distance(X, old_centroids)\n",
    "        return self.find_closest_cluster(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geyser eruptions segmentation (2D dataset)\n",
    "# Modules\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.datasets.samples_generator import (make_blobs,\n",
    "                                                make_circles,\n",
    "                                                make_moons)\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_context('notebook')\n",
    "plt.style.use('fivethirtyeight')\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "# Import the data\n",
    "df = pd.read_csv('../data/old_faithful.csv')\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(df.iloc[:, 0], df.iloc[:, 1])\n",
    "plt.xlabel('Eruption time in mins')\n",
    "plt.ylabel('Waiting time to next eruption')\n",
    "plt.title('Visualization of raw data');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "X_std = StandardScaler().fit_transform(df)\n",
    "\n",
    "# Run local implementation of kmeans\n",
    "km = Kmeans(n_clusters=2, max_iter=100)\n",
    "km.fit(X_std)\n",
    "centroids = km.centroids\n",
    "\n",
    "# Plot the clustered data\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "plt.scatter(X_std[km.labels == 0, 0], X_std[km.labels == 0, 1],\n",
    "            c='green', label='cluster 1')\n",
    "plt.scatter(X_std[km.labels == 1, 0], X_std[km.labels == 1, 1],\n",
    "            c='blue', label='cluster 2')\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], marker='*', s=300,\n",
    "            c='r', label='centroid')\n",
    "plt.legend()\n",
    "plt.xlim([-2, 2])\n",
    "plt.ylim([-2, 2])\n",
    "plt.xlabel('Eruption time in mins')\n",
    "plt.ylabel('Waiting time to next eruption')\n",
    "plt.title('Visualization of clustered data', fontweight='bold')\n",
    "ax.set_aspect('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 9\n",
    "fig, ax = plt.subplots(3, 3, figsize=(16, 16))\n",
    "ax = np.ravel(ax)\n",
    "centers = []\n",
    "for i in range(n_iter):\n",
    "    # Run local implementation of kmeans\n",
    "    km = Kmeans(n_clusters=2,\n",
    "                max_iter=3,\n",
    "                random_state=np.random.randint(0, 1000, size=1))\n",
    "    km.fit(X_std)\n",
    "    centroids = km.centroids\n",
    "    centers.append(centroids)\n",
    "    ax[i].scatter(X_std[km.labels == 0, 0], X_std[km.labels == 0, 1],\n",
    "                  c='green', label='cluster 1')\n",
    "    ax[i].scatter(X_std[km.labels == 1, 0], X_std[km.labels == 1, 1],\n",
    "                  c='blue', label='cluster 2')\n",
    "    ax[i].scatter(centroids[:, 0], centroids[:, 1],\n",
    "                  c='r', marker='*', s=300, label='centroid')\n",
    "    ax[i].set_xlim([-2, 2])\n",
    "    ax[i].set_ylim([-2, 2])\n",
    "    ax[i].legend(loc='lower right')\n",
    "    ax[i].set_title(f'{km.error:.4f}')\n",
    "    ax[i].set_aspect('equal')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation methods\n",
    "#Elbow method\n",
    "# Run the Kmeans algorithm and get the index of data points clusters\n",
    "sse = []\n",
    "list_k = list(range(1, 10))\n",
    "\n",
    "for k in list_k:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km.fit(X_std)\n",
    "    sse.append(km.inertia_)\n",
    "\n",
    "# Plot sse against k\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(list_k, sse, '-o')\n",
    "plt.xlabel(r'Number of clusters *k*')\n",
    "plt.ylabel('Sum of squared distance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Silhouette analysis\n",
    "for i, k in enumerate([2, 3, 4]):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "    \n",
    "    # Run the Kmeans algorithm\n",
    "    km = KMeans(n_clusters=k)\n",
    "    labels = km.fit_predict(X_std)\n",
    "    centroids = km.cluster_centers_\n",
    "\n",
    "    # Get silhouette samples\n",
    "    silhouette_vals = silhouette_samples(X_std, labels)\n",
    "\n",
    "    # Silhouette plot\n",
    "    y_ticks = []\n",
    "    y_lower, y_upper = 0, 0\n",
    "    for i, cluster in enumerate(np.unique(labels)):\n",
    "        cluster_silhouette_vals = silhouette_vals[labels == cluster]\n",
    "        cluster_silhouette_vals.sort()\n",
    "        y_upper += len(cluster_silhouette_vals)\n",
    "        ax1.barh(range(y_lower, y_upper), cluster_silhouette_vals, edgecolor='none', height=1)\n",
    "        ax1.text(-0.03, (y_lower + y_upper) / 2, str(i + 1))\n",
    "        y_lower += len(cluster_silhouette_vals)\n",
    "\n",
    "    # Get the average silhouette score and plot it\n",
    "    avg_score = np.mean(silhouette_vals)\n",
    "    ax1.axvline(avg_score, linestyle='--', linewidth=2, color='green')\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    ax1.set_xlabel('Silhouette coefficient values')\n",
    "    ax1.set_ylabel('Cluster labels')\n",
    "    ax1.set_title('Silhouette plot for the various clusters', y=1.02);\n",
    "    \n",
    "    # Scatter plot of data colored with labels\n",
    "    ax2.scatter(X_std[:, 0], X_std[:, 1], c=labels)\n",
    "    ax2.scatter(centroids[:, 0], centroids[:, 1], marker='*', c='r', s=250)\n",
    "    ax2.set_xlim([-2, 2])\n",
    "    ax2.set_xlim([-2, 2])\n",
    "    ax2.set_xlabel('Eruption time in mins')\n",
    "    ax2.set_ylabel('Waiting time to next eruption')\n",
    "    ax2.set_title('Visualization of clustered data', y=1.02)\n",
    "    ax2.set_aspect('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'Silhouette analysis using k = {k}',\n",
    "                 fontsize=16, fontweight='semibold', y=1.05);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-k-means-clustering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardizing the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# statistics of scaled data\n",
    "pd.DataFrame(data_scaled).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the kmeans function with initialization as k-means++\n",
    "kmeans = KMeans(n_clusters=2, init='k-means++')\n",
    "\n",
    "# fitting the k means algorithm on scaled data\n",
    "kmeans.fit(data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inertia on the fitted data\n",
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting multiple k-means algorithms and storing the values in an empty list\n",
    "SSE = []\n",
    "for cluster in range(1,20):\n",
    "    kmeans = KMeans(n_jobs = -1, n_clusters = cluster, init='k-means++')\n",
    "    kmeans.fit(data_scaled)\n",
    "    SSE.append(kmeans.inertia_)\n",
    "\n",
    "# converting the results into a dataframe and plotting them\n",
    "frame = pd.DataFrame({'Cluster':range(1,20), 'SSE':SSE})\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(frame['Cluster'], frame['SSE'], marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k means using 5 clusters and k-means++ initialization\n",
    "kmeans = KMeans(n_jobs = -1, n_clusters = 5, init='k-means++')\n",
    "kmeans.fit(data_scaled)\n",
    "pred = kmeans.predict(data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.DataFrame(data_scaled)\n",
    "frame['cluster'] = pred\n",
    "frame['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## other clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# The coordinates of the centers of our 3 blobs.\n",
    "centers = [[2, 2], [-2, -2], [2, -2]]\n",
    "\n",
    "# Make 10,000 rows worth of data with two features representing three\n",
    "# clusters, each having a standard deviation of 1.\n",
    "X, y = make_blobs(\n",
    "    n_samples=10000,\n",
    "    centers=centers,\n",
    "    cluster_std=1,\n",
    "    n_features=2,\n",
    "    random_state=42)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.show()\n",
    "\n",
    "#Divide into training and test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.9,\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data.\n",
    "#X_norm = normalize(X)\n",
    "\n",
    "# Reduce it to two components.\n",
    "X_pca = PCA(2).fit_transform(X)\n",
    "\n",
    "# Calculate predicted values.\n",
    "y_pred = KMeans(n_clusters=3, random_state=42).fit_predict(X_pca)\n",
    "\n",
    "# Plot the solution.\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_pred)\n",
    "plt.show()\n",
    "\n",
    "# Check the solution against the data.\n",
    "print('Comparing k-means clusters against the data:')\n",
    "print(pd.crosstab(y_pred, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each batch will be made up of 200 data points.\n",
    "minibatchkmeans = MiniBatchKMeans(\n",
    "    init='random',\n",
    "    n_clusters=3,\n",
    "    batch_size=200)\n",
    "minibatchkmeans.fit(X_pca)\n",
    "\n",
    "# Predict new MiniBatch cluster memberships.\n",
    "predict_mini = minibatchkmeans.predict(X_pca)\n",
    "\n",
    "# Check the MiniBatch model against our earlier one.\n",
    "print('Comparing k-means and mini batch k-means solutions:')\n",
    "print(pd.crosstab(predict_mini, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "\n",
    "# Here we set the bandwidth. This function automatically derives a bandwidth\n",
    "# number based on an inspection of the distances among points in the data.\n",
    "bandwidth = estimate_bandwidth(X_train, quantile=0.2, n_samples=500)\n",
    "\n",
    "# Declare and fit the model.\n",
    "ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "ms.fit(X_train)\n",
    "\n",
    "# Extract cluster assignments for each data point.\n",
    "labels = ms.labels_\n",
    "\n",
    "# Coordinates of the cluster centers.\n",
    "cluster_centers = ms.cluster_centers_\n",
    "\n",
    "# Count our clusters.\n",
    "n_clusters_ = len(np.unique(labels))\n",
    "\n",
    "print(\"Number of estimated clusters: {}\".format(n_clusters_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=labels)\n",
    "plt.show()\n",
    "\n",
    "print('Comparing the assigned categories to the ones in the data:')\n",
    "print(pd.crosstab(y_train,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "# We know we're looking for three clusters.\n",
    "n_clusters=3\n",
    "\n",
    "# Declare and fit the model.\n",
    "sc = SpectralClustering(n_clusters=n_clusters)\n",
    "sc.fit(X_train)\n",
    "\n",
    "#Predicted clusters.\n",
    "predict=sc.fit_predict(X_train)\n",
    "\n",
    "#Graph results.\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=predict)\n",
    "plt.show()\n",
    "\n",
    "print('Comparing the assigned categories to the ones in the data:')\n",
    "print(pd.crosstab(y_train,predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn import metrics\n",
    "\n",
    "# Declare the model and fit it in one statement.\n",
    "# Note that you can provide arguments to the model, but we didn't.\n",
    "af = AffinityPropagation().fit(X_train)\n",
    "print('Done')\n",
    "\n",
    "# Pull the number of clusters and cluster assignments for each data point.\n",
    "cluster_centers_indices = af.cluster_centers_indices_\n",
    "n_clusters_ = len(cluster_centers_indices)\n",
    "labels = af.labels_\n",
    "\n",
    "print('Estimated number of clusters: {}'.format(n_clusters_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "\n",
    "# Cycle through each cluster and graph them with a center point for the\n",
    "# exemplar and lines from the exemplar to each data point in the cluster.\n",
    "colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')\n",
    "for k, col in zip(range(n_clusters_), colors):\n",
    "    class_members = labels == k\n",
    "    cluster_center = X_train[cluster_centers_indices[k]]\n",
    "    plt.plot(X_train[class_members, 0], X_train[class_members, 1], col + '.')\n",
    "    plt.plot(cluster_center[0],\n",
    "             cluster_center[1],\n",
    "             'o',\n",
    "             markerfacecolor=col,\n",
    "             markeredgecolor='k')\n",
    "    for x in X_train[class_members]:\n",
    "        plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)\n",
    "\n",
    "plt.title('Estimated number of clusters: {}'.format(n_clusters_))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "data_path = (\"https://tf-assets-prod.s3.amazonaws.com/tf-curric/\"\n",
    "             \"data-science/cleveland-data.csv\")\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Make sure the number of rows divides evenly into four samples.\n",
    "rows = df.shape[0] - df.shape[0] % 4\n",
    "df = df.iloc[:rows, :]\n",
    "\n",
    "# Break into a set of features and a variable for the known outcome.\n",
    "X = df.iloc[:, :13]\n",
    "y = df.iloc[:, 13]\n",
    "\n",
    "# Replace some random string values.\n",
    "X = X.replace(to_replace='?', value=0)\n",
    "\n",
    "# Binarize y so that 1 means heart disease diagnosis and 0 means no diagnosis.\n",
    "y = np.where(y > 0, 0, 1)\n",
    "\n",
    "# Normalize\n",
    "X_norm = normalize(X)\n",
    "\n",
    "# Data frame to store features and predicted cluster memberships.\n",
    "ypred = pd.DataFrame()\n",
    "\n",
    "# Create the two-feature PCA for graphing purposes.\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_norm)\n",
    "\n",
    "# Split the data into four equally-sized samples. First we break it in half:\n",
    "X_half1, X_half2, X_pcahalf1, X_pcahalf2 = train_test_split(\n",
    "    X_norm,\n",
    "    X_pca,\n",
    "    test_size=0.5,\n",
    "    random_state=42)\n",
    "\n",
    "# Then we halve the halves.\n",
    "X1, X2, X_pca1, X_pca2 = train_test_split(\n",
    "    X_half1,\n",
    "    X_pcahalf1,\n",
    "    test_size=0.5,\n",
    "    random_state=42)\n",
    "X3, X4, X_pca3, X_pca4 = train_test_split(\n",
    "    X_half2,\n",
    "    X_pcahalf2,\n",
    "    test_size=0.5,\n",
    "    random_state=42)\n",
    "\n",
    "# Pass a list of tuples and a counter that increments each time we go\n",
    "# through the loop. The tuples are the data to be used by k-means,\n",
    "# and the PCA-derived features for graphing. We use k-means to fit a\n",
    "# model to the data, then store the predicted values and the two-feature\n",
    "# PCA solution in the data frame.\n",
    "for counter, data in enumerate([\n",
    "    (X1, X_pca1),\n",
    "    (X2, X_pca2),\n",
    "    (X3, X_pca3),\n",
    "    (X4, X_pca4)]):\n",
    "    \n",
    "    # Put the features into ypred.\n",
    "    ypred['pca_f1' + '_sample' + str(counter)] = data[1][:, 0]\n",
    "    ypred['pca_f2' + '_sample' + str(counter)] = data[1][:, 1]\n",
    "    \n",
    "    # Generate cluster predictions and store them for clusters 2 to 4.\n",
    "    for nclust in range(2, 5):\n",
    "        pred = KMeans(n_clusters=nclust, random_state=42).fit_predict(data[0])\n",
    "        ypred['clust' + str(nclust) + '_sample' + str(counter)] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each  number of clusters, plot the clusters using the\n",
    "# pca features for each sample.\n",
    "for cluster in range(2, 5):\n",
    "    \n",
    "    # Make a grid of subplots.\n",
    "    f, axarr = plt.subplots(2, 2)\n",
    "    \n",
    "    # Make a plot for each sample.\n",
    "    for i in range(4):\n",
    "        \n",
    "        # PCA-created features.\n",
    "        x_sub = ypred['pca_f1_sample{}'.format(i)]\n",
    "        y_sub = ypred['pca_f2_sample{}'.format(i)]\n",
    "        \n",
    "        # Cluster assignments.\n",
    "        c = ypred['clust{}_sample{}'.format(cluster, i)]\n",
    "        \n",
    "        # Assign the subplot to its place on the grid.\n",
    "        rows = int(np.floor(i / 2))\n",
    "        cols = i % 2\n",
    "        axarr[rows, cols].scatter(x_sub, y_sub, c=c)\n",
    "        axarr[rows, cols].set_title('sample {}'.format(i))\n",
    "        axarr[rows, cols].set_xlim([-.3, .3])\n",
    "        axarr[rows, cols].set_ylim([-.3, .3])\n",
    "    \n",
    "    # Space out the plots so that the headings don't overlap axis values.\n",
    "    plt.suptitle('{} Clusters'.format(cluster), fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted clusters.\n",
    "full_pred = KMeans(n_clusters=2, random_state=42).fit_predict(X_norm)\n",
    "\n",
    "pd.crosstab(y, full_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: the itertools module is tons of fun and very useful.\n",
    "import itertools\n",
    "\n",
    "# Create a list of pairs, where each pair is the ground truth group\n",
    "# and the assigned cluster.\n",
    "c = list(itertools.product(y, full_pred))\n",
    "\n",
    "# Count how often each type of pair (a, b, c, or d) appears.\n",
    "RIcounts = [[x, c.count(x)] for x in set(c)]\n",
    "print(RIcounts)\n",
    "\n",
    "# Create the same counts but without the label, for easier math below.\n",
    "RIcounts_nolabel = [c.count(x) for x in set(c)]\n",
    "\n",
    "# Calculate the Rand Index.\n",
    "RIscore = (RIcounts_nolabel[3] + RIcounts_nolabel[2]) / np.sum(RIcounts_nolabel)\n",
    "print(RIscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "    \n",
    "metrics.adjusted_rand_score(y, full_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "for sample in [X1, X2, X3, X4]:\n",
    "    model = KMeans(n_clusters=2, random_state=42).fit(sample)\n",
    "    labels = model.labels_\n",
    "    print(metrics.silhouette_score(sample, labels, metric='euclidean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
